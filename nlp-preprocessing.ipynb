{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Pipeline Notebook\n\n#### **Motivation:** A Genric functions and pipline that can fit into our project (I want my life to be easy).\n\nIt includes:\n\n- Reading input text\n- preprocessing\n- Tokenization and POS tagging\n- Dependency parsing\n- Named Entity Recognition (NER)\n- Sentiment Analysis using both TextBlob and VADER\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Environment Setup\n","metadata":{}},{"cell_type":"code","source":"# !pip install spacy textblob nltk\n# !python -m textblob.download_corpora\n# !python -m spacy download en_core_web_sm\n# !python -m nltk.downloader vader_lexicon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:33.940915Z","iopub.execute_input":"2025-04-09T18:20:33.941273Z","iopub.status.idle":"2025-04-09T18:20:33.944969Z","shell.execute_reply.started":"2025-04-09T18:20:33.941241Z","shell.execute_reply":"2025-04-09T18:20:33.943819Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### My Little Explanation:\n- **spaCy**: A fast and production-ready library for NLP tasks.\n- **TextBlob**: Provides an easy API for basic NLP tasks, e.g. sentiment analysis.\n- **nltk**: A classic NLP library. We'll use its VADER sentiment analyzer.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Import Packages and Load Models\n","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom textblob import TextBlob\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport re\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Download required NLTK resources (if not already downloaded)\n# nltk.download('punkt')\n# nltk.download('stopwords')\nnltk.download('wordnet')\n# nltk.download('omw-1.4')  # For WordNet lemmatizer language support\n\n\n# Load the spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Download VADER lexicon for sentiment analysis (if not already downloaded)\n# nltk.download(\"vader_lexicon\")\nsia = SentimentIntensityAnalyzer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:39.196302Z","iopub.execute_input":"2025-04-09T18:20:39.196667Z","iopub.status.idle":"2025-04-09T18:20:46.011975Z","shell.execute_reply.started":"2025-04-09T18:20:39.196636Z","shell.execute_reply":"2025-04-09T18:20:46.011172Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# import nltk\n# nltk.download('punkt', quiet=True)\n# nltk.download('stopwords', quiet=True)\n# nltk.download('wordnet', quiet=True)\n# nltk.download('omw-1.4', quiet=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:48.135081Z","iopub.execute_input":"2025-04-09T18:20:48.135666Z","iopub.status.idle":"2025-04-09T18:20:48.139380Z","shell.execute_reply.started":"2025-04-09T18:20:48.135627Z","shell.execute_reply":"2025-04-09T18:20:48.138373Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# nltk.data.path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:48.421717Z","iopub.execute_input":"2025-04-09T18:20:48.422070Z","iopub.status.idle":"2025-04-09T18:20:48.425957Z","shell.execute_reply.started":"2025-04-09T18:20:48.422041Z","shell.execute_reply":"2025-04-09T18:20:48.424912Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\ndef nltk_preprocess_text(text, lower=True, remove_numbers=True, remove_punct=True, \n                           remove_stopwords=True, use_lemmatization=False, use_stemming=False,\n                           extra_clean=True):\n    \"\"\"\n    Preprocesses input text using NLTK.\n    \n    The function performs the following steps:\n    1. **Extra Cleaning:** Removes extra spaces and, optionally, unwanted characters via regex.\n    2. **Lowercasing:** Converts the text to lowercase.\n    3. **Tokenization:** Splits the text into tokens using NLTK's word_tokenize.\n    4. **Punctuation & Number Removal:** Filters out tokens that are punctuation or numbers.\n    5. **Stop Word Removal:** Removes common stop words using NLTK's stop word list.\n    6. **Lemmatization:** Converts tokens to their base forms using the WordNetLemmatizer.\n    7. **Stemming (Optional):** Optionally applies PorterStemmer to tokens after lemmatization.\n    \n    Parameters:\n      - text (str): The raw input text.\n      - lower (bool): If True, converts text to lowercase. Default: True.\n      - remove_numbers (bool): If True, removes tokens that are digits. Default: True.\n      - remove_punct (bool): If True, removes tokens that are non-alphanumeric. Default: True.\n      - remove_stopwords (bool): If True, filters out common stop words. Default: True.\n      - use_lemmatization (bool): If True, uses WordNetLemmatizer on tokens. Default: True.\n      - use_stemming (bool): If True, applies PorterStemmer to tokens after lemmatization. Default: False.\n      - extra_clean (bool): If True, performs extra regex cleaning to remove unwanted characters and extra spaces. Default: True.\n      \n    Returns:\n      str: The cleaned, preprocessed text.\n    \"\"\"\n    # Extra cleaning using regex: remove extra spaces and non-alphanumeric characters if desired.\n    if extra_clean:\n        text = re.sub(r'\\s+', ' ', text)      # Replace multiple spaces with one space.\n        text = re.sub(r'[^\\w\\s]', ' ', text)   # Remove punctuation characters.\n    \n    # Lowercase conversion\n    if lower:\n        text = text.lower()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Define stop words set\n    stops = set(stopwords.words('english'))\n    \n    # Initialize lemmatizer and stemmer objects\n    wnl = WordNetLemmatizer()\n    ps = PorterStemmer()\n    \n    processed_tokens = []\n    for token in tokens:\n        # Optionally remove tokens that are punctuation (i.e., non-alphanumeric)\n        if remove_punct and not token.isalnum():\n            continue\n        # Optionally remove numbers\n        if remove_numbers and token.isdigit():\n            continue\n        # Optionally remove stop words\n        if remove_stopwords and token in stops:\n            continue\n        \n        # Apply lemmatization (get the base form of the word)\n        if use_lemmatization:\n            token = wnl.lemmatize(token)\n        # Optionally apply stemming after lemmatization\n        if use_stemming:\n            token = ps.stem(token)\n        \n        # Add token if it's not empty\n        if token.strip():\n            processed_tokens.append(token.strip())\n    \n    return \" \".join(processed_tokens)\n\n\n\n\n# --- Example usage ---\nsample_text = (\"Apple Inc. is planning to launch a new iPhone this September! \"\n               \"Visit www.apple.com for details in 2023. Running, runner, runs.\")\n\nprint(\"Original Text:\")\nprint(sample_text)\n\nprint(\"\\nProcessed Text (no lemm no stemming):\")\nprint(nltk_preprocess_text(sample_text, use_lemmatization=False, use_stemming=False))\n\nprint(\"\\nProcessed Text (only  Stemming):\")\nprint(nltk_preprocess_text(sample_text, use_lemmatization=False, use_stemming=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:50.556690Z","iopub.execute_input":"2025-04-09T18:20:50.557028Z","iopub.status.idle":"2025-04-09T18:20:50.585790Z","shell.execute_reply.started":"2025-04-09T18:20:50.557000Z","shell.execute_reply":"2025-04-09T18:20:50.584866Z"}},"outputs":[{"name":"stdout","text":"Original Text:\nApple Inc. is planning to launch a new iPhone this September! Visit www.apple.com for details in 2023. Running, runner, runs.\n\nProcessed Text (no lemm no stemming):\napple inc planning launch new iphone september visit www apple com details running runner runs\n\nProcessed Text (only  Stemming):\nappl inc plan launch new iphon septemb visit www appl com detail run runner run\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 3: Reading Input Text\n","metadata":{}},{"cell_type":"code","source":"def read_input_file(file_path):\n    \"\"\"\n    Reads text from a given file path.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return f.read().strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:55.804243Z","iopub.execute_input":"2025-04-09T18:20:55.804573Z","iopub.status.idle":"2025-04-09T18:20:55.809051Z","shell.execute_reply.started":"2025-04-09T18:20:55.804544Z","shell.execute_reply":"2025-04-09T18:20:55.807839Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Step 4: Tokenization and Part-of-Speech Tagging\n","metadata":{}},{"cell_type":"code","source":"def tokenize_and_tag(text):\n    \"\"\"\n    Tokenizes and tags the text, returning tokens with their POS tags.\n    \"\"\"\n    doc = nlp(text)\n    return [(token.text, token.pos_) for token in doc]\n\nexample_text = \"Apple Inc. is planning to launch a new iPhone this September.\"\ntokenize_and_tag(example_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:56.252918Z","iopub.execute_input":"2025-04-09T18:20:56.253314Z","iopub.status.idle":"2025-04-09T18:20:56.289597Z","shell.execute_reply.started":"2025-04-09T18:20:56.253279Z","shell.execute_reply":"2025-04-09T18:20:56.288640Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[('Apple', 'PROPN'),\n ('Inc.', 'PROPN'),\n ('is', 'AUX'),\n ('planning', 'VERB'),\n ('to', 'PART'),\n ('launch', 'VERB'),\n ('a', 'DET'),\n ('new', 'ADJ'),\n ('iPhone', 'PROPN'),\n ('this', 'DET'),\n ('September', 'PROPN'),\n ('.', 'PUNCT')]"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Step 5: Dependency Parsing\n\n#### we parse the sentence to understand the grammatical relationships between words.\n","metadata":{}},{"cell_type":"code","source":"# one note if you dont know dependency_parse please read it out, it we'll not take even 30 min (this note is for me only)\n\ndef dependency_parse(text):\n    \"\"\"\n    Returns the dependency relations for each token.\n    \"\"\"\n    doc = nlp(text)\n    return [(token.text, token.dep_, token.head.text) for token in doc]\n\ndependency_parse(example_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:20:58.019667Z","iopub.execute_input":"2025-04-09T18:20:58.020036Z","iopub.status.idle":"2025-04-09T18:20:58.038274Z","shell.execute_reply.started":"2025-04-09T18:20:58.020002Z","shell.execute_reply":"2025-04-09T18:20:58.037244Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[('Apple', 'compound', 'Inc.'),\n ('Inc.', 'nsubj', 'planning'),\n ('is', 'aux', 'planning'),\n ('planning', 'ROOT', 'planning'),\n ('to', 'aux', 'launch'),\n ('launch', 'xcomp', 'planning'),\n ('a', 'det', 'iPhone'),\n ('new', 'amod', 'iPhone'),\n ('iPhone', 'dobj', 'launch'),\n ('this', 'det', 'September'),\n ('September', 'npadvmod', 'launch'),\n ('.', 'punct', 'planning')]"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### Explanation:\n- The `dependency_parse` function shows the relationship (like subject, object, etc.) between each token and its head word.\n- This helps in understanding how parts of the sentence relate to each other.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 6: Named Entity Recognition (NER)","metadata":{}},{"cell_type":"code","source":"def named_entities(text):\n    \"\"\"\n    Extracts named entities from the text.\n    \"\"\"\n    doc = nlp(text)\n    return [(ent.text, ent.label_) for ent in doc.ents]\n\n\nnamed_entities(example_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:21:00.324579Z","iopub.execute_input":"2025-04-09T18:21:00.324919Z","iopub.status.idle":"2025-04-09T18:21:00.343362Z","shell.execute_reply.started":"2025-04-09T18:21:00.324891Z","shell.execute_reply":"2025-04-09T18:21:00.342405Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[('Apple Inc.', 'ORG'), ('iPhone', 'ORG'), ('this September', 'DATE')]"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Step 7: Sentiment Analysis\n\nWe'll perform sentiment analysis using both TextBlob and VADER.\n","metadata":{}},{"cell_type":"code","source":"def sentiment_textblob(text):\n    \"\"\"\n    Uses TextBlob to analyze the sentiment of the text.\n    Returns polarity and subjectivity.\n    \"\"\"\n    blob = TextBlob(text)\n    return blob.sentiment.polarity, blob.sentiment.subjectivity\n\ndef sentiment_vader(text):\n    \"\"\"\n    Uses nltk's VADER to analyze sentiment.\n    Returns a dictionary with sentiment scores.\n    \"\"\"\n    return sia.polarity_scores(text)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:21:02.449288Z","iopub.execute_input":"2025-04-09T18:21:02.449638Z","iopub.status.idle":"2025-04-09T18:21:02.454644Z","shell.execute_reply.started":"2025-04-09T18:21:02.449607Z","shell.execute_reply":"2025-04-09T18:21:02.453462Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"example_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:21:02.679281Z","iopub.execute_input":"2025-04-09T18:21:02.679598Z","iopub.status.idle":"2025-04-09T18:21:02.684975Z","shell.execute_reply.started":"2025-04-09T18:21:02.679572Z","shell.execute_reply":"2025-04-09T18:21:02.684097Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'Apple Inc. is planning to launch a new iPhone this September.'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"print(\"TextBlob Sentiment:\", sentiment_textblob(example_text))\nprint(\"VADER Sentiment:\", sentiment_vader(example_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:21:02.873136Z","iopub.execute_input":"2025-04-09T18:21:02.873496Z","iopub.status.idle":"2025-04-09T18:21:02.923812Z","shell.execute_reply.started":"2025-04-09T18:21:02.873469Z","shell.execute_reply":"2025-04-09T18:21:02.922761Z"}},"outputs":[{"name":"stdout","text":"TextBlob Sentiment: (0.13636363636363635, 0.45454545454545453)\nVADER Sentiment: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Explanation:\n- **TextBlob** returns sentiment polarity ([-1, 1]) and subjectivity ([0, 1]).  \n- **VADER** is particularly tuned for social media and news text, returning scores for positive, negative, neutral, and compound sentiment.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 8: Full NLP Pipeline Function\n\n","metadata":{}},{"cell_type":"code","source":"def run_pipeline_with_preprocessing(text):\n    \"\"\"\n    Runs the full NLP pipeline after preprocessing the input text.\n    \"\"\"\n    print(\"----- Raw Input -----\")\n    print(text)\n    \n    preprocessed_text = nltk_preprocess_text(text, lower=True, remove_numbers=True, remove_punct=True, \n                           remove_stopwords=False, use_stemming=False,\n                           extra_clean=True)\n    \n    print(\"\\n----- Preprocessed Text -----\")\n    print(preprocessed_text)\n    \n    print(\"\\n Tokens & POS Tags:\")\n    tokens_pos = tokenize_and_tag(preprocessed_text)\n    print(tokens_pos)\n    \n    print(\"\\n Dependency Parse:\")\n    dependencies = dependency_parse(preprocessed_text)\n    print(dependencies)\n    \n    print(\"\\n Named Entities:\")\n    entities = named_entities(preprocessed_text)\n    print(entities)\n    \n    print(\"\\n Sentiment Analysis (TextBlob):\")\n    polarity, subjectivity = sentiment_textblob(preprocessed_text)\n    print(f\"Polarity: {polarity}, Subjectivity: {subjectivity}\")\n    \n    print(\"\\n Sentiment Analysis (VADER):\")\n    vader_scores = sentiment_vader(preprocessed_text)\n    print(vader_scores)\n\n\ninput_text = 'The weather in Bengaluru today is perfect for a walk in the park'  #positive \n# input_text = 'VADER is not smart, handsome, nor funny.' #negative\nrun_pipeline_with_preprocessing(input_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:21:04.930675Z","iopub.execute_input":"2025-04-09T18:21:04.931012Z","iopub.status.idle":"2025-04-09T18:21:04.970756Z","shell.execute_reply.started":"2025-04-09T18:21:04.930969Z","shell.execute_reply":"2025-04-09T18:21:04.969696Z"}},"outputs":[{"name":"stdout","text":"----- Raw Input -----\nThe weather in Bengaluru today is perfect for a walk in the park\n\n----- Preprocessed Text -----\nthe weather in bengaluru today is perfect for a walk in the park\n\n Tokens & POS Tags:\n[('the', 'DET'), ('weather', 'NOUN'), ('in', 'ADP'), ('bengaluru', 'NOUN'), ('today', 'NOUN'), ('is', 'AUX'), ('perfect', 'ADJ'), ('for', 'ADP'), ('a', 'DET'), ('walk', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('park', 'NOUN')]\n\n Dependency Parse:\n[('the', 'det', 'weather'), ('weather', 'nsubj', 'is'), ('in', 'prep', 'weather'), ('bengaluru', 'pobj', 'in'), ('today', 'npadvmod', 'is'), ('is', 'ROOT', 'is'), ('perfect', 'acomp', 'is'), ('for', 'prep', 'is'), ('a', 'det', 'walk'), ('walk', 'pobj', 'for'), ('in', 'prep', 'walk'), ('the', 'det', 'park'), ('park', 'pobj', 'in')]\n\n Named Entities:\n[('today', 'DATE')]\n\n Sentiment Analysis (TextBlob):\nPolarity: 1.0, Subjectivity: 1.0\n\n Sentiment Analysis (VADER):\n{'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'compound': 0.5719}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}