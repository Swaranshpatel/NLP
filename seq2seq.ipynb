{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Project Structure\n",
    "\n",
    "\n",
    "### We’re going to create a mini-projects:\n",
    "\n",
    "- Seq2Seq with Attention for Machine Translation\n",
    "\n",
    "#### project will cover:\n",
    "\n",
    "- Dataset selection and preparation \n",
    "\n",
    "- Model building\n",
    "\n",
    "- Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing this code for lstm and gru and setting  complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:08:32.784016Z",
     "iopub.status.busy": "2025-04-15T10:08:32.783195Z",
     "iopub.status.idle": "2025-04-15T10:08:32.788378Z",
     "shell.execute_reply": "2025-04-15T10:08:32.787542Z",
     "shell.execute_reply.started": "2025-04-15T10:08:32.783990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn #for loss fn and neural network\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:08:35.116978Z",
     "iopub.status.busy": "2025-04-15T10:08:35.116686Z",
     "iopub.status.idle": "2025-04-15T10:08:35.120971Z",
     "shell.execute_reply": "2025-04-15T10:08:35.120358Z",
     "shell.execute_reply.started": "2025-04-15T10:08:35.116956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "\n",
    "batch_size=64\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "num_layers = 2 \n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:08:56.783242Z",
     "iopub.status.busy": "2025-04-15T10:08:56.782965Z",
     "iopub.status.idle": "2025-04-15T10:08:57.952219Z",
     "shell.execute_reply": "2025-04-15T10:08:57.951680Z",
     "shell.execute_reply.started": "2025-04-15T10:08:56.783222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#dataset \n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, text, word2idx=None, idx2word=None, seq_len=5):\n",
    "        words = text.strip().split()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Only build vocab if not provided\n",
    "        if word2idx is None or idx2word is None:\n",
    "            vocab = sorted(set(words))\n",
    "            self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "            self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        else:\n",
    "            self.word2idx = word2idx\n",
    "            self.idx2word = idx2word\n",
    "\n",
    "        # Make sure all words exist in vocab\n",
    "        self.data = [self.word2idx[word] for word in words if word in self.word2idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx + self.seq_len])\n",
    "        y = torch.tensor(self.data[idx + 1:idx + self.seq_len + 1])\n",
    "        return x, y\n",
    "# Build vocab from training data only\n",
    "base_dataset = WordDataset(' '.join(train_lines[:50000]))\n",
    "word2idx = base_dataset.word2idx\n",
    "idx2word = base_dataset.idx2word\n",
    "\n",
    "# Pass vocab to other splits\n",
    "train_dataset = base_dataset\n",
    "val_dataset = WordDataset(' '.join(val_lines), word2idx, idx2word)\n",
    "test_dataset = WordDataset(' '.join(test_lines), word2idx, idx2word)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size*20, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:09:30.473537Z",
     "iopub.status.busy": "2025-04-15T10:09:30.472988Z",
     "iopub.status.idle": "2025-04-15T10:09:30.476956Z",
     "shell.execute_reply": "2025-04-15T10:09:30.476330Z",
     "shell.execute_reply.started": "2025-04-15T10:09:30.473513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:09:32.513741Z",
     "iopub.status.busy": "2025-04-15T10:09:32.513150Z",
     "iopub.status.idle": "2025-04-15T10:09:32.527042Z",
     "shell.execute_reply": "2025-04-15T10:09:32.526380Z",
     "shell.execute_reply.started": "2025-04-15T10:09:32.513718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class UniversalRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,\n",
    "                 mode='rnn', dropout=0.1,\n",
    "                 device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super(UniversalRNNModel, self).__init__()\n",
    "        \n",
    "        assert mode in ['rnn', 'lstm', 'gru'], \"Mode must be one of: 'rnn', 'lstm', 'gru'\"\n",
    "        self.mode = mode\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        rnn_class = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}[mode]\n",
    "        self.rnn = rnn_class(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.003)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        if self.mode == 'lstm':\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "            return (h0, c0)\n",
    "        else:\n",
    "            return h0\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(self.norm(x), hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.tanh(out.reshape(-1, self.hidden_dim))\n",
    "        out = self.fc2(self.relu(self.dropout(self.fc1(out))))\n",
    "        out = out.reshape(x.shape[0], x.shape[1], -1)\n",
    "        return out, hidden\n",
    "\n",
    "    def train_step(self, loader):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        prog_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "        for x, y in prog_bar:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            hidden = self.init_hidden(x.size(0))\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "            logits, _ = self(x, hidden)\n",
    "            loss = self.criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (prog_bar.n + 1)\n",
    "            prog_bar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "    \n",
    "        return total_loss / len(loader)\n",
    "\n",
    "    def val_step(self, loader):\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        correct_preds = 0\n",
    "    \n",
    "        prog_bar = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for x, y in prog_bar:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                hidden = self.init_hidden(x.size(0))\n",
    "                logits, _ = self(x, hidden)\n",
    "    \n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_tokens += x.numel()\n",
    "    \n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                correct_preds += (preds == y).sum().item()\n",
    "    \n",
    "                avg_loss = total_loss / (prog_bar.n * loader.batch_size + x.size(0))\n",
    "                prog_bar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "    \n",
    "        avg_loss = total_loss / len(loader.dataset)\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        accuracy = correct_preds / total_tokens\n",
    "    \n",
    "        return avg_loss, perplexity, accuracy\n",
    "\n",
    "    def test_step(self, loader):\n",
    "        return self.val_step(loader)\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        x = x.to(self.device)\n",
    "        hidden = self.init_hidden(x.size(0))\n",
    "        logits, _ = self(x, hidden)\n",
    "        output = torch.argmax(logits, dim=-1)[:, 0]\n",
    "        return output, torch.cat((x, output.unsqueeze(1)), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:12:22.843430Z",
     "iopub.status.busy": "2025-04-15T10:12:22.843182Z",
     "iopub.status.idle": "2025-04-15T10:12:22.847851Z",
     "shell.execute_reply": "2025-04-15T10:12:22.847257Z",
     "shell.execute_reply.started": "2025-04-15T10:12:22.843412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n🌀 Epoch {epoch}/{epochs}\")\n",
    "        train_loss = model.train_step(train_loader)\n",
    "        val_loss, ppl, acc = model.val_step(val_loader)\n",
    "        \n",
    "        print(f\"📉 Train Loss: {train_loss:.4f} | 🔍 Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"🧠 Val Loss: {val_loss:.4f} | 🤯 Perplexity: {ppl:.2f} | 🎯 Accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:12:23.116602Z",
     "iopub.status.busy": "2025-04-15T10:12:23.116023Z",
     "iopub.status.idle": "2025-04-15T10:12:23.120547Z",
     "shell.execute_reply": "2025-04-15T10:12:23.119848Z",
     "shell.execute_reply.started": "2025-04-15T10:12:23.116577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:12:24.501637Z",
     "iopub.status.busy": "2025-04-15T10:12:24.500999Z",
     "iopub.status.idle": "2025-04-15T10:12:24.982884Z",
     "shell.execute_reply": "2025-04-15T10:12:24.982315Z",
     "shell.execute_reply.started": "2025-04-15T10:12:24.501614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_rnn = UniversalRNNModel(vocab_size, embedding_dim, hidden_dim, num_layers, mode='rnn').to(device)\n",
    "model_lstm = UniversalRNNModel(vocab_size, embedding_dim, hidden_dim, num_layers, mode='lstm').to(device)\n",
    "model_gru = UniversalRNNModel(vocab_size, embedding_dim, hidden_dim, num_layers, mode='gru').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:06:04.756348Z",
     "iopub.status.busy": "2025-04-15T11:06:04.755740Z",
     "iopub.status.idle": "2025-04-15T11:06:04.762526Z",
     "shell.execute_reply": "2025-04-15T11:06:04.761954Z",
     "shell.execute_reply.started": "2025-04-15T11:06:04.756319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, word2idx, idx2word, seed_text, max_length=50, device=None):\n",
    "    \"\"\"\n",
    "    Generate text using a trained SimpleRNN model.\n",
    "\n",
    "    Args:\n",
    "        model (SimpleRNN): Trained model.\n",
    "        word2idx (dict): Mapping from word to index.\n",
    "        idx2word (dict): Mapping from index to word.\n",
    "        seed_text (str): Seed input text.\n",
    "        max_length (int): Total length of the generated sequence.\n",
    "        device: Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text sequence.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = device or model.device\n",
    "    model.to(device)\n",
    "\n",
    "    words = seed_text.strip().split()\n",
    "    input_ids = [word2idx.get(w, word2idx[list(word2idx.keys())[0]]) for w in words]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # shape: (1, seq_len)\n",
    "\n",
    "    hidden = model.init_hidden(1)\n",
    "    generated = input_tensor.clone()\n",
    "\n",
    "    for _ in range(max_length - len(input_ids)):\n",
    "        logits, hidden = model(generated, hidden)\n",
    "        next_token_logits = logits[:, -1, :]  # shape: (1, vocab_size)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1)  # shape: (1,)\n",
    "        generated = torch.cat((generated, next_token.unsqueeze(1)), dim=1)\n",
    "\n",
    "    generated_words = [idx2word[idx] for idx in generated[0].tolist()]\n",
    "    return ' '.join(generated_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:06:48.440868Z",
     "iopub.status.busy": "2025-04-15T11:06:48.440373Z",
     "iopub.status.idle": "2025-04-15T11:06:48.508061Z",
     "shell.execute_reply": "2025-04-15T11:06:48.507479Z",
     "shell.execute_reply.started": "2025-04-15T11:06:48.440845Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who had also worked on the album . The song was released in the United States . The first two @-@ year @-@ old Sammo Hung , and the first\n",
      "who had also worked on the album 's first episode of the song \" The \" \" \" The \" \" \" \" \" \" \" \" \" \" \"\n"
     ]
    }
   ],
   "source": [
    "# text = generate_text(model_rnn, word2idx = word2idx, idx2word = idx2word, seed_text=\"who had also worked on the\", max_length=30)\n",
    "# print(text)\n",
    "text = generate_text(model_lstm, word2idx = word2idx, idx2word = idx2word, seed_text=\"who had also worked on the\", max_length=30)\n",
    "print(text)\n",
    "text = generate_text(model_gru, word2idx = word2idx, idx2word = idx2word, seed_text=\"who had also worked on the\", max_length=30)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 823358,
     "sourceId": 1408148,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
