{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1423,"sourceType":"datasetVersion","datasetId":747},{"sourceId":2730445,"sourceType":"datasetVersion","datasetId":1167113}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install gensim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:51:39.022679Z","iopub.execute_input":"2025-04-14T11:51:39.023476Z","iopub.status.idle":"2025-04-14T11:51:39.034435Z","shell.execute_reply.started":"2025-04-14T11:51:39.023447Z","shell.execute_reply":"2025-04-14T11:51:39.030341Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Our Objective\nTraining **Word2Vec** and **GloVe** embeddings on a custom dataset(wikibooks and quora), and comparing their ability to capture semantic relationships through multiple downstream tasks (like word similarity and analogy tasks).","metadata":{}},{"cell_type":"code","source":"## preprocessing\n\n# Clean text: lowercase, remove punctuation, stopwords, etc.\n# Tokenize and prepare the corpus for embedding training.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:51:41.629332Z","iopub.execute_input":"2025-04-14T11:51:41.629658Z","iopub.status.idle":"2025-04-14T11:51:41.634090Z","shell.execute_reply.started":"2025-04-14T11:51:41.629631Z","shell.execute_reply":"2025-04-14T11:51:41.633104Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\nimport pandas as pd \nimport gensim\nfrom gensim.models import Word2Vec\n\n\n\nimport sqlite3\nfrom gensim.utils import simple_preprocess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:51:41.635060Z","iopub.execute_input":"2025-04-14T11:51:41.635379Z","iopub.status.idle":"2025-04-14T11:51:55.932978Z","shell.execute_reply.started":"2025-04-14T11:51:41.635350Z","shell.execute_reply":"2025-04-14T11:51:55.932077Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import logging\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:51:55.938188Z","iopub.execute_input":"2025-04-14T11:51:55.938535Z","iopub.status.idle":"2025-04-14T11:51:55.943371Z","shell.execute_reply.started":"2025-04-14T11:51:55.938502Z","shell.execute_reply":"2025-04-14T11:51:55.942273Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Establish a connection to the SQLite database\nconn = sqlite3.connect('/kaggle/input/wikibooks-dataset/wikibooks.sqlite')\n\n# Create a cursor object\ncursor = conn.cursor()\n\n# Execute the SQL query to retrieve table names\ncursor.execute(\"SELECT * from en\")\n\n# Fetch all the table names\nraw_eng_text = cursor.fetchall()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:51:55.944456Z","iopub.execute_input":"2025-04-14T11:51:55.944725Z","iopub.status.idle":"2025-04-14T11:52:43.838445Z","shell.execute_reply.started":"2025-04-14T11:51:55.944703Z","shell.execute_reply":"2025-04-14T11:52:43.837553Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"cursor.execute(f\"PRAGMA table_info(en);\")\ncolumn_names = cursor.fetchall()\ncolumn_names = [column[1] for column in column_names]\n\n# Create a pandas DataFrame from the fetched data\ndf_eng_text = pd.DataFrame(raw_eng_text, columns=column_names)\n \n    \ndf_eng_text.body_text.str.len().max()\nlen(df_eng_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:52:43.839425Z","iopub.execute_input":"2025-04-14T11:52:43.839683Z","iopub.status.idle":"2025-04-14T11:52:44.531024Z","shell.execute_reply.started":"2025-04-14T11:52:43.839661Z","shell.execute_reply":"2025-04-14T11:52:44.529986Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"86736"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"sub_df = df_eng_text['body_text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:52:44.532594Z","iopub.execute_input":"2025-04-14T11:52:44.532991Z","iopub.status.idle":"2025-04-14T11:52:44.538877Z","shell.execute_reply.started":"2025-04-14T11:52:44.532957Z","shell.execute_reply":"2025-04-14T11:52:44.537245Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"sub_df ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:52:44.540116Z","iopub.execute_input":"2025-04-14T11:52:44.540476Z","iopub.status.idle":"2025-04-14T11:52:44.569437Z","shell.execute_reply.started":"2025-04-14T11:52:44.540438Z","shell.execute_reply":"2025-04-14T11:52:44.568405Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0        Front Page: Radiation Oncology | RTOG Trials |...\n1        Băuturi/Beverages[edit | edit source]\\nTea : C...\n2        Karrigell is an open Source Python web framewo...\n3        setupUnitPanel[edit | edit source]\\nHelper fun...\n4        Contents\\n\\n1 The Concept\\n2 The System\\n3 The...\n                               ...                        \n86731    Previous: Self Help\\n\\nIndex\\n\\nNext: Variable...\n86732    ← Contributing\\n\\nCalculus\\n\\nAlgebra →\\n\\n\\nP...\n86733    There are 11 castles in Somerset.\\n\\n\\n\\n\\nNam...\n86734    Contents\\n\\n1 CULTURAL STUDIES AND IDENTITY\\n\\...\n86735    Sardine is a nutritious oily fish.\\n沙丁鱼是一种有营养的...\nName: body_text, Length: 86736, dtype: object"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/question-pairs-dataset/questions.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:52:44.570343Z","iopub.execute_input":"2025-04-14T11:52:44.570658Z","iopub.status.idle":"2025-04-14T11:52:46.031166Z","shell.execute_reply.started":"2025-04-14T11:52:44.570624Z","shell.execute_reply":"2025-04-14T11:52:46.030104Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"texts = [*df['question1'].values.tolist(), *df['question1'].values.tolist(), *sub_df.values.tolist()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:52:46.034642Z","iopub.execute_input":"2025-04-14T11:52:46.035003Z","iopub.status.idle":"2025-04-14T11:52:46.111743Z","shell.execute_reply.started":"2025-04-14T11:52:46.034975Z","shell.execute_reply":"2025-04-14T11:52:46.110765Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"texts = [[word for word in word_tokenize(str(sentance)) if word.isalnum()] for sentance in texts]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:52:46.112584Z","iopub.execute_input":"2025-04-14T11:52:46.112840Z","iopub.status.idle":"2025-04-14T12:05:46.680438Z","shell.execute_reply.started":"2025-04-14T11:52:46.112821Z","shell.execute_reply":"2025-04-14T12:05:46.679132Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"cbow_model = Word2Vec(texts, vector_size=100, window=5, min_count=1, sg=0, alpha=0.03, min_alpha=0.0007, epochs=100)\nskipgram_model = Word2Vec(texts, vector_size=100, window=5, min_count=1, sg=1, alpha=0.03, min_alpha=0.0007, epochs=100)\n\ncbow_model.train(texts, total_examples=len(texts), epochs=100)\nskipgram_model.train(texts, total_examples=len(texts), epochs=100)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:05:46.681317Z","iopub.execute_input":"2025-04-14T12:05:46.681577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# texts\n\ncbow_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_vectors_cbow = cbow_model.wv\nsimilarity_cbow = word_vectors_cbow.similarity('invest', 'market')\nprint(f\"Similarity between 'invest', 'market': {similarity_cbow} with CBOW\")\n\n\nword_vectors_skipgram= skipgram_model.wv\nsimilarity_skip = word_vectors_skipgram.similarity('invest', 'market')\nprint(f\"Similarity between 'invest', 'market': {similarity_skip} with Skip-Gram\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef similarity_score(word1, word2, model, is_glove=False):\n    try:\n        if is_glove:\n            vec1 = model.word_vectors[model.dictionary[word1]]\n            vec2 = model.word_vectors[model.dictionary[word2]]\n        else:\n            vec1 = model.wv[word1]\n            vec2 = model.wv[word2]\n        return cosine_similarity([vec1], [vec2])[0][0]\n    except:\n        return None\n\nwords_to_test = [('king', 'queen'), ('apple', 'banana'), ('paris', 'france')]\n\nfor w1, w2 in words_to_test:\n    print(f\"\\n{w1} - {w2}\")\n    print(\"CBOW:\", similarity_score(w1, w2, cbow_model))\n    print(\"Skip-gram:\", similarity_score(w1, w2, skipgram_model))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}